# ü§ó Hugging Face API Integration Guide

This guide will help you set up Hugging Face API integration for your AI chatbot, giving you access to thousands of open-source AI models.

## üìã What is Hugging Face?

Hugging Face is a platform that provides:
- **Thousands of open-source AI models** (free to use)
- **Easy API access** to state-of-the-art models
- **Multiple model types**: text generation, conversation, translation, etc.
- **Cost-effective alternative** to commercial AI services

## üîë Step 1: Get Your Hugging Face API Token

1. **Visit Hugging Face**
   - Go to [https://huggingface.co](https://huggingface.co)
   - Sign up or log in to your account

2. **Create API Token**
   - Click on your profile picture ‚Üí "Settings"
   - Go to "Access Tokens" in the sidebar
   - Click "New token"
   - Give it a name (e.g., "AI Chatbot")
   - Select "Read" permissions
   - Click "Generate token"
   - Copy the token (starts with `hf_`)

## ‚öôÔ∏è Step 2: Configure Your Environment

1. **Add to Environment File**
   ```bash
   # Edit .env.local and add your Hugging Face token
   HUGGINGFACE_API_KEY=hf_your_token_here
   ```

2. **Restart Your App**
   ```bash
   npm run dev
   ```

## üöÄ Step 3: Available Models

Your chatbot now supports these Hugging Face models:

### **Conversational Models**
- **DialoGPT Medium** (`microsoft/DialoGPT-medium`)
  - Great for chat conversations
  - Fast and reliable
  - Good for general conversation

- **DialoGPT Large** (`microsoft/DialoGPT-large`)
  - More capable than Medium
  - Better response quality
  - Slightly slower

- **BlenderBot 400M** (`facebook/blenderbot-400M-distill`)
  - Open-domain conversations
  - Good for diverse topics
  - Facebook's conversational AI

### **Instruction-Following Models**
- **Flan-T5 Base** (`google/flan-t5-base`)
  - Good for following instructions
  - Task-oriented responses
  - Google's instruction model

- **Flan-T5 Large** (`google/flan-t5-large`)
  - More capable than Base
  - Better instruction following
  - Higher quality responses

### **Text Generation Models**
- **BLOOM 560M** (`bigscience/bloom-560m`)
  - Multilingual support
  - Good for various languages
  - Open-source alternative to GPT

- **GPT-2** (`gpt2`)
  - Classic OpenAI model
  - Good for creative text
  - Reliable performance

- **GPT-Neo 125M** (`EleutherAI/gpt-neo-125M`)
  - Modern GPT alternative
  - Good performance
  - Open-source

## üéØ Step 4: Using Hugging Face Models

### **In the Chat Interface**
1. Click the model selector dropdown
2. Choose any Hugging Face model
3. Start chatting - the model will respond using the selected AI

### **Model Selection Tips**
- **For conversations**: Use DialoGPT models
- **For instructions**: Use Flan-T5 models
- **For creative text**: Use GPT-2 or GPT-Neo
- **For multiple languages**: Use BLOOM

## üí∞ Cost Comparison

### **Hugging Face (Free Tier)**
- **API Calls**: 30,000 requests/month (free)
- **Model Loading**: Free
- **Inference**: Free
- **Rate Limits**: Reasonable limits

### **OpenAI (Paid)**
- **GPT-3.5-turbo**: ~$0.002 per 1K tokens
- **GPT-4**: ~$0.06 per 1K tokens
- **No free tier** for API usage

### **When to Use Each**
- **Use Hugging Face**: For cost-effective, open-source AI
- **Use OpenAI**: For highest quality, commercial-grade responses

## üîß Step 5: Advanced Configuration

### **Model Parameters**
You can customize model behavior:

```typescript
// In your API call
{
  modelId: 'microsoft/DialoGPT-medium',
  maxTokens: 1000,        // Maximum response length
  temperature: 0.7,       // Creativity (0.0-1.0)
  topP: 0.9,             // Nucleus sampling
  doSample: true         // Enable sampling
}
```

### **Adding Custom Models**
To add your own Hugging Face models:

1. **Edit `lib/ai-service.ts`**
2. **Add to MODELS array**:
   ```typescript
   {
     id: 'your-model-id',
     name: 'Your Model Name',
     provider: 'huggingface',
     description: 'Your model description',
     maxTokens: 512,
     temperature: 0.7
   }
   ```

## üö® Troubleshooting

### **"Model not found" Error**
- Check the model ID is correct
- Ensure the model exists on Hugging Face
- Verify the model supports text generation

### **"API key not configured" Error**
- Check your `HUGGINGFACE_API_KEY` environment variable
- Ensure the token starts with `hf_`
- Restart your development server

### **"Rate limit exceeded" Error**
- Hugging Face has rate limits on free tier
- Wait a few minutes and try again
- Consider upgrading to paid plan for higher limits

### **"Model loading failed" Error**
- Some models take time to load on first use
- Wait and try again
- Check if the model is available

### **Slow Responses**
- Hugging Face models load on-demand
- First request to a model may be slower
- Subsequent requests will be faster

## üìä Monitoring Usage

### **Hugging Face Dashboard**
- Visit [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
- Check your token usage
- Monitor rate limits

### **In Your App**
- Check browser console for API errors
- Monitor response times
- Track model performance

## üîí Security Best Practices

1. **Token Security**
   - Never commit your token to version control
   - Use environment variables
   - Rotate tokens regularly

2. **Rate Limiting**
   - Implement client-side rate limiting
   - Handle rate limit errors gracefully
   - Monitor usage patterns

3. **Model Validation**
   - Validate model IDs before use
   - Handle model loading errors
   - Provide fallback options

## üöÄ Performance Optimization

### **Model Caching**
- Hugging Face caches models after first load
- Subsequent requests are faster
- Consider model warm-up strategies

### **Response Optimization**
- Set appropriate `maxTokens` limits
- Use `temperature` for creativity control
- Implement streaming for long responses

### **Error Handling**
- Implement retry logic for failed requests
- Provide fallback to other models
- Show user-friendly error messages

## üìö Additional Resources

- **Hugging Face Documentation**: [https://huggingface.co/docs](https://huggingface.co/docs)
- **Model Hub**: [https://huggingface.co/models](https://huggingface.co/models)
- **Inference API**: [https://huggingface.co/inference-api](https://huggingface.co/inference-api)
- **Community**: [https://huggingface.co/community](https://huggingface.co/community)

## üéâ Benefits of Hugging Face Integration

### **Cost Savings**
- Free tier with generous limits
- No per-token charges
- Access to thousands of models

### **Model Variety**
- Conversational models
- Instruction-following models
- Multilingual models
- Specialized models

### **Open Source**
- Transparent model behavior
- Community-driven improvements
- No vendor lock-in

### **Flexibility**
- Easy to switch between models
- Customizable parameters
- Extensible architecture

---

üéâ **Congratulations!** Your AI chatbot now supports both OpenAI and Hugging Face models, giving you the best of both worlds - commercial-grade AI and cost-effective open-source alternatives! 